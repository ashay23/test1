{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lxz7-0Hm4sXC"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "class cleaningPipeline():\n",
        "    def _init_(self):\n",
        "        pass\n",
        "\n",
        "    def remove_non_ascii(self, text):\n",
        "        \"\"\"Remove non-ASCII characters from the text\"\"\"\n",
        "        text = re.sub(r'\\x85', '', text)  # replace ellipses\n",
        "        text = re.sub(r'\\x91', '', text)  # replace left single quote\n",
        "        text = re.sub(r'\\x92', '', text)  # replace right single quote\n",
        "        text = re.sub(u'\\x93', '', text)  # replace left double quote\n",
        "        text = re.sub(u'\\x94', '', text)  # replace right double quote\n",
        "        text = re.sub(r'\\x95', '', text)  # replace bullet\n",
        "        text = re.sub(r'\\x96', '', text)  # replace bullet\n",
        "        text = re.sub(r'\\x99', '', text)  # replace TM\n",
        "        text = re.sub(r'\\xae', '', text)  # replace (R)\n",
        "        text = re.sub(r'\\xb0', '', text)  # replace degree symbol\n",
        "        text = re.sub(r'\\xba', '', text)  # replace degree symbol\n",
        "        new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        return new_text\n",
        "\n",
        "    def remove_whitespace_and_special_chars(self, text):\n",
        "        '''Remove whitespace and special characters from the text'''\n",
        "        tab_newline_pattern = '[\\t\\n]'  # regex for newline\n",
        "        multi_space = ' {2,}'  # regex for multispace\n",
        "\n",
        "        formatted_text = text.lower()  # lower all the text\n",
        "\n",
        "        formatted_text = re.sub(r'(?:\\d{1,2} )?(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]* (?:\\d{1,2}, )?\\d{2,4}', 'mdate', formatted_text)\n",
        "        formatted_text = re.sub(r'(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]* \\d{1,2}[a-z]*', 'mdate', formatted_text)\n",
        "\n",
        "        formatted_text = re.sub('(?<=\\d),(?=\\d)', 'commaseperatednum', formatted_text)\n",
        "        formatted_text = re.sub(tab_newline_pattern, ' ', formatted_text)\n",
        "        formatted_text = re.sub(multi_space, ' ', formatted_text)\n",
        "        formatted_text = re.compile(r'[^a-zA-Z0-9\\s]').sub(' ', formatted_text)\n",
        "        formatted_text = re.sub(multi_space, ' ', formatted_text)\n",
        "        return formatted_text\n",
        "\n",
        "    def remove_numerics(self, text):\n",
        "        '''Remove all numeric values from the text'''\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        return text\n",
        "\n",
        "    def removeStopWord(self, text):\n",
        "        '''Removes all stopwords e.g., a, the, etc...'''\n",
        "        stop = set(stopwords.words('english'))\n",
        "        return \" \".join([word for word in word_tokenize(text) if word.lower() not in stop])\n",
        "\n",
        "    def word_lemmatization(self, text):\n",
        "        '''Lemmatize words in the text'''\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        words = word_tokenize(text)\n",
        "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "        lemmatized_text = \" \".join(lemmatized_words)\n",
        "        return lemmatized_text\n",
        "\n",
        "    def transform(self, text):\n",
        "        '''Call above methods in sequence'''\n",
        "        if not isinstance(text, str):\n",
        "            text = str(text)\n",
        "        text = self.remove_non_ascii(text)\n",
        "        text = self.remove_whitespace_and_special_chars(text)\n",
        "        text = self.removeStopWord(text)\n",
        "        text = self.word_lemmatization(text)\n",
        "        text = self.remove_numerics(text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "vGc27CC74xDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def save_to_pickle(dataframe, filename):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(dataframe, file)\n",
        "    print(f'DataFrame saved to {filename}')"
      ],
      "metadata": {
        "id": "cGVPQoWE49wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "class VectorizationPipeline():\n",
        "    def __init__(self, max_features=5000):\n",
        "        self.vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "\n",
        "    def fit_transform(self, text_data):\n",
        "        return self.vectorizer.fit_transform(text_data)\n",
        "\n",
        "    def transform(self, text_data):\n",
        "        return self.vectorizer.transform(text_data)\n",
        "\n",
        "    def get_feature_names(self):\n",
        "        return self.vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "XNXBQ-rP5ZvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "def save_vectorization_pipeline(filename, max_features=5000):\n",
        "    vectorization_pipeline = VectorizationPipeline(max_features=max_features)\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(vectorization_pipeline, file)\n",
        "    print(f'VectorizationPipeline saved to {filename}')\n",
        "\n",
        "# Save the vectorization pipeline\n",
        "save_vectorization_pipeline('vectorization_pipeline.pkl')"
      ],
      "metadata": {
        "id": "Erf00_xk6gbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b419a3a3-93cb-419e-f100-d0b826d21b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VectorizationPipeline saved to vectorization_pipeline.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8g7dZ7Q_zoZx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}